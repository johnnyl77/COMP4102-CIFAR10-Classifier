{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOwMz8P4u8xf"
      },
      "source": [
        "### Install the `torchmetrics` library for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfRu1q4UrZLv",
        "outputId": "d016358f-d01b-42c6-942f-caf7dbc7037a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchmetrics in c:\\users\\johnn\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.8.2)\n",
            "Requirement already satisfied: numpy>1.20.0 in c:\\users\\johnn\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchmetrics) (2.2.6)\n",
            "Requirement already satisfied: packaging>17.1 in c:\\users\\johnn\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in c:\\users\\johnn\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchmetrics) (2.8.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in c:\\users\\johnn\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchmetrics) (0.15.2)\n",
            "Requirement already satisfied: setuptools in c:\\users\\johnn\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (80.9.0)\n",
            "Requirement already satisfied: typing_extensions in c:\\users\\johnn\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.13.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\johnn\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.18.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\johnn\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (1.14.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\johnn\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\johnn\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in c:\\users\\johnn\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (2025.9.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\johnn\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\johnn\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WvyBoe94rICo"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tqdm'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchmetrics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclassification\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Accuracy\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m resnet50, ResNet50_Weights, vgg16, VGG16_Weights\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tqdm'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchmetrics.classification import Accuracy\n",
        "from torchvision.models import resnet50, ResNet50_Weights, vgg16, VGG16_Weights\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tz_NcLjEr7TR",
        "outputId": "9574db97-a4fd-47c9-c4fb-c416713952a1"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rhFBjZ0ZsATj"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 5\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.0001\n",
        "NUM_CLASSES = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "W0h-ONwLrvHM"
      },
      "outputs": [],
      "source": [
        "def load_data(T, batch_size):\n",
        "  train_data = datasets.CIFAR10(root='./data', train=True, download=True, transform=T)\n",
        "  train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  test_data = datasets.CIFAR10(root='./data', train=False, download=True, transform=T)\n",
        "  test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RegnwjvarUWX"
      },
      "outputs": [],
      "source": [
        "def plot_results(train_losses, test_losses, test_accuracies):\n",
        "  \"\"\"\n",
        "  Plot the train and test losses and accuracy values over a number of epochs.\n",
        "  \"\"\"\n",
        "  fig, ax = plt.subplots(1, 2, figsize=(9, 3))\n",
        "\n",
        "  ax[0].plot(train_losses, label='Train Loss')\n",
        "  ax[0].plot(test_losses, label='Test Loss', color='red')\n",
        "  ax[0].legend()\n",
        "  ax[1].plot([value for value in test_accuracies], label='Test Accuracy', color='green')\n",
        "\n",
        "  ax[0].set_title('Train / Test Loss')\n",
        "  ax[1].set_title('Test Accuracy')\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3F3qoAtu6Qsj"
      },
      "source": [
        "# Q1. Complete the following implementations:\n",
        "- `train_epoch`\n",
        "- `eval_model`\n",
        "- `run_experiment`\n",
        "- `SimpleCNN`\n",
        "- `ResNet50Transfer`\n",
        "- `VGGTransfer`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCKsMz9UzaCo"
      },
      "source": [
        "In the following functions, you are training and evaluating the model for just one epoch (One full pass through the dataset). Complete the `train_epoch` and `eval_model` functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LUEIwHDsrj3K"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
        "  \"\"\"\n",
        "  Train the model for one epoch.\n",
        "\n",
        "  args:\n",
        "    model: torch.nn.Module, the model to train\n",
        "    train_loader: torch.utils.data.DataLoader, the dataloader for the training set\n",
        "    optimizer: torch.optim.Optimizer, the optimizer to use\n",
        "    criterion: torch.nn.Module, the loss function to use\n",
        "    device: torch.device\n",
        "\n",
        "  returns:\n",
        "    train_loss: float, the average loss over the training set for this epoch\n",
        "  \"\"\"\n",
        "  running_loss = 0.0\n",
        "  total_batches = len(train_loader) # Total batches of samples in training set\n",
        "\n",
        "  # set the model to training e.g. enable dropouts\n",
        "  model.train(mode=True)\n",
        "\n",
        "  # Iterate through the batch of samples in the train set\n",
        "  for inputs, labels in tqdm(train_loader):\n",
        "    # zero the previously calculated gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # make predictions\n",
        "    inputs,labels = inputs.to(device), labels.to(device)\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    # calculate the loss\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # calculate the gradient of the loss\n",
        "    loss.backward()\n",
        "\n",
        "    # adjust the model parameters based on the gradient\n",
        "    optimizer.step()\n",
        "\n",
        "    # keep track of the loss\n",
        "    running_loss += loss.item()\n",
        "\n",
        "  # return the average of the loss over all of the batches\n",
        "  return running_loss / total_batches\n",
        "\n",
        "def eval_model(model, test_loader, criterion, acc, device):\n",
        "\n",
        "  \"\"\"\n",
        "  Evaluate the model on the test set.\n",
        "\n",
        "  args:\n",
        "    model: torch.nn.Module, the model to evaluate\n",
        "    test_loader: torch.utils.data.DataLoader, the dataloader for the test set\n",
        "    criterion: torch.nn.Module, the loss function to use\n",
        "    acc: torchmetrics.Accuracy, the accuracy metric object\n",
        "    device: torch.device\n",
        "\n",
        "  returns:\n",
        "    test_loss: float, the average loss over the test set\n",
        "    test_acc: float, the accuracy over the test set\n",
        "  \"\"\"\n",
        "\n",
        "  running_loss = 0.0\n",
        "  total_batches = len(test_loader) # Total batches of samples in testing set\n",
        "  # set the model to eval mode e.g. disable dropouts\n",
        "  model.eval()\n",
        "  # reset the calculated accuracy object\n",
        "  acc.reset()\n",
        "  acc\n",
        "\n",
        "  # make sure the following does not affect the gradient calculation\n",
        "  with torch.no_grad():\n",
        "    # Iterate through the batch of samples in the test set\n",
        "    for inputs, labels in tqdm(test_loader, desc=\"Testing Batch Samples\"):\n",
        "      # make predictions\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      outputs = model(inputs)\n",
        "      # calculate the loss\n",
        "      loss = criterion(outputs, labels)\n",
        "      # keep track of the loss\n",
        "      running_loss += loss.item()\n",
        "      # update the overall accuracy by including the accuracy of this batch\n",
        "      acc.update(outputs, labels)\n",
        "\n",
        "  # return:\n",
        "    # the average loss over the test set\n",
        "    # the accuracy over the test set\n",
        "  return running_loss / total_batches, acc.compute().item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhLDqkJ71V_z"
      },
      "source": [
        "In the following function you will use the previous function to train the model over a number of epochs. Complete the `run_experiment` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QDk_1Cy4rn-f"
      },
      "outputs": [],
      "source": [
        "def run_experiment(model, train_loader, test_loader, optimizer, criterion, device, epochs=10):\n",
        "  \"\"\"\n",
        "  Run the experiment for a given number of epochs.\n",
        "\n",
        "  args:\n",
        "    model: torch.nn.Module, the model to train\n",
        "    train_loader: torch.utils.data.DataLoader, the dataloader for the training set\n",
        "    test_loader: torch.utils.data.DataLoader, the dataloader for the test set\n",
        "    optimizer: torch.optim.Optimizer, the optimizer to use\n",
        "    criterion: torch.nn.Module, the loss function to use\n",
        "    device: torch.device\n",
        "    epochs: int, the number of epochs to train for\n",
        "\n",
        "  returns:\n",
        "    train_losses: list of floats, the train loss for each epoch\n",
        "    test_losses: list of floats, the test loss for each epoch\n",
        "    accuracies: list of floats, the test accuracy for each epoch\n",
        "  \"\"\"\n",
        "  train_losses = []\n",
        "  test_losses = []\n",
        "  accuracies = []\n",
        "\n",
        "  acc = Accuracy('multiclass', num_classes=NUM_CLASSES).to(device) # Initialize the accuracy object\n",
        "\n",
        "  # Interate through the number of epochs\n",
        "  for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch}/{epochs}: \")\n",
        "    # pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
        "    # Train the model (using the `train_epoch` function)\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    # Keep track of the training loss for this epoch\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Evaluate the model (using the `eval_model` function)\n",
        "    test_loss, test_acc = eval_model(model, test_loader, criterion, acc, device)\n",
        "    # Keep track of the testing loss and the testing accuracy for this epoch\n",
        "    test_losses.append(test_loss)\n",
        "    accuracies.append(test_acc)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "  print(\"Finished training.\")\n",
        "\n",
        "  # return:\n",
        "    # train_losses: list of floats, the train loss for each epoch\n",
        "    # test_losses: list of floats, the test loss for each epoch\n",
        "    # accuracies: list of floats, the test accuracy for each epoch\n",
        "  return train_losses, test_losses, accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nKRXfSc874Fq"
      },
      "outputs": [],
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "  def __init__(self, num_classes=10):\n",
        "    super().__init__()\n",
        "\n",
        "\n",
        "    # Conv2d (# of input channels ('64' --> 128) so 64 in this case\n",
        "    #         # of output channels (64 --> '128'), so 128 in this case\n",
        "    #         kernel size (3x3, so 3),\n",
        "    #         1 pixel border around input to preserve 32x32 img otherwise it becomes 30x30\n",
        "\n",
        "    # ReLU\n",
        "    # Allows non-linearity in network\n",
        "    # Mitigates the vanishing gradient\n",
        "\n",
        "    # Maxpooling2d\n",
        "    # Reduces computational cost through downsampling / reducing size of the feature map while maintaining the important features\n",
        "    # kernel_size=2, so it's a 2x2 sliding window\n",
        "    # stride=2, so it's the distance that the sliding window moves vertically & horizontally when sampling\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "      # Convolution(3x3 kernel) channels: 3 -> 32\n",
        "      nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "      # ReLU\n",
        "      nn.ReLU(),\n",
        "      # Max pooling (2x2)\n",
        "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "\n",
        "      # Convolution(3x3 kernel) channels: 32 -> 64\n",
        "      nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "      # ReLU\n",
        "      nn.ReLU(),\n",
        "      # Max pooling (2x2)\n",
        "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "      # Convolution(3x3 kernel) channels: 64 -> 128\n",
        "      nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "      # ReLU\n",
        "      nn.ReLU(),\n",
        "      # Max pooling (2x2)\n",
        "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "\n",
        "      nn.Flatten(),\n",
        "      # Linear 512 hidden_dim\n",
        "      nn.Linear(128*4*4, 512),\n",
        "      # ReLU\n",
        "      nn.ReLU(),\n",
        "      # Linear output -> num_classes\n",
        "      nn.Linear(512, num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vTLYSkq20oDz"
      },
      "outputs": [],
      "source": [
        "class ResNet50Transfer(nn.Module):\n",
        "  def __init__(self, num_classes=10):\n",
        "    super().__init__()\n",
        "\n",
        "    # Initialize Resnet 50 with the default weights\n",
        "    self.resnet = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
        "\n",
        "    # Freeze the resnet weights\n",
        "    self.resnet.requires_grad_(False)\n",
        "\n",
        "    # replace the last fully connected layers with the following:\n",
        "    self.resnet.fc = nn.Sequential(\n",
        "      # Linear 256 hidden_dim\n",
        "      nn.Linear(self.resnet.fc.in_features, 256),\n",
        "      # ReLU\n",
        "      nn.ReLU(),\n",
        "      # Linear output -> num_classes\n",
        "      nn.Linear(256, num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.resnet(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "gCF0z7C0rp0-"
      },
      "outputs": [],
      "source": [
        "class VGGTransfer(nn.Module):\n",
        "  def __init__(self, num_classes=10):\n",
        "    super().__init__()\n",
        "\n",
        "    # Initialize VGG16 with the default weights\n",
        "    self.vgg = vgg16(weights=VGG16_Weights.DEFAULT)\n",
        "    # Freeze the feature weights\n",
        "    self.vgg.requires_grad_(False)\n",
        "\n",
        "    # Replace the classifier with the following:\n",
        "    self.vgg.classifier = nn.Sequential(\n",
        "      nn.Flatten(),\n",
        "      # Linear 256 hidden_dim\n",
        "      nn.Linear(512 * 7 * 7, 256),\n",
        "      # ReLU\n",
        "      nn.ReLU(),\n",
        "      # Linear output -> num_classes\n",
        "      nn.Linear(256, num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.vgg(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snkOINeA7EFA"
      },
      "source": [
        "# Q2. Run the following experiments and report the results.\n",
        "- Train and evaluate the `SimpleCNN` model on the CIFAR-10 dataset (for 5 epochs minimum).\n",
        "- Plot the train and test loss, as well as the test accuracy.\n",
        "- Analyze and report the results. Provide reasoning for your analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpSn6YBF8L8s",
        "outputId": "63e27cc8-3fed-4ea5-8af9-e619c38eeeb3"
      },
      "outputs": [],
      "source": [
        "T = transforms.Compose([\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_loader, test_loader = load_data(T, BATCH_SIZE)\n",
        "simple_cnn = SimpleCNN(NUM_CLASSES).to(device)\n",
        "optimizer = torch.optim.Adam(simple_cnn.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "train_losses, test_losses, accuracies = run_experiment(simple_cnn, train_loader, test_loader, optimizer, criterion, device, EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "sGk5KidK8RN5",
        "outputId": "af98fbc1-c0de-4630-fef2-c76b1c01c5b7"
      },
      "outputs": [],
      "source": [
        "plot_results(train_losses, test_losses, accuracies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hxt_-3dRLQU"
      },
      "source": [
        "The results of running the SimpleCNN on the CIFAR-10 dataset through 5 epochs show that\n",
        "\n",
        "1.   The training loss gradually lowers from around 1.70 to 1.11\n",
        "2.   The test loss gradually decreases from around 1.44 to 1.12\n",
        "3.   The test accuracy improves from around 0.48 to 0.60\n",
        "\n",
        "Through observing all of this data we can see that the SimpleCNN model is learning effectively since both the training and test losses start far apart and converge during epoch 3 to 4. Additionally, since the losses are converging versus diverging, it shows that the there is no overfitting.\n",
        "\n",
        "The accuracy does not seem to be that high because after 5 epochs it seems to be that the accuracy start peaking around 60% which is not reliable when you think about it realistically.\n",
        "\n",
        "We can conclude that the SimpleCNN is learning well without overfitting, but the accuracy of this model is too low, suggesting that a better model could be used to achieve a higher accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vukI4spGsJl"
      },
      "source": [
        "# Q3. Run the following experiments and report the results.\n",
        "- Train and evaluate the `ResNet50_Weights` model on the CIFAR-10 dataset (for 5 epochs minimum).\n",
        "- Plot the train and test loss, as well as the test accuracy.\n",
        "- Analyze and report the results. Provide reasoning for your analysis. Make comparisons to the previous experiments when appropriate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsAPC2s5060V",
        "outputId": "04b2932a-897c-45c2-d571-e204ccb8a1c2"
      },
      "outputs": [],
      "source": [
        "T = ResNet50_Weights.DEFAULT.transforms()\n",
        "train_loader, test_loader = load_data(T, BATCH_SIZE)\n",
        "\n",
        "resnet_transfer = ResNet50Transfer(NUM_CLASSES).to(device)\n",
        "optimizer = torch.optim.Adam(resnet_transfer.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "train_losses, test_losses, accuracies = run_experiment(resnet_transfer, train_loader, test_loader, optimizer, criterion, device, EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "g3jRsgSQ1GR4",
        "outputId": "85f1662e-d876-4f83-9085-0124fdb57b6d"
      },
      "outputs": [],
      "source": [
        "plot_results(train_losses, test_losses, accuracies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4Wr8xtUWeBA"
      },
      "source": [
        "The results of running the ResNet50 Model on the CIFAR-10 dataset through 5 epochs show that\n",
        "\n",
        "1.   The training loss very quickly drops from over 1.05 to 0.54\n",
        "2.   The test loss gradually decreases from 0.71 to 0.56\n",
        "3.   The test accuracy improves from 0.77 to 0.81\n",
        "\n",
        "Through observing all of this data we can see that the ResNet50 loss already initializes at a lower training and test loss and quickly drops decreases in both training loss and test loss within the first epoch of training. Additionally, we can see that the losses are converging much earlier at epoch 3 instead of 4 which shows quicker initial learning with the model. The final losses for the ResNet50 model are both lower compared to the SimpleCNN model.\n",
        "\n",
        "The accuracy here ends up initializing at 0.77 which is already quite high and jumps to 0.81 which is a significant improvement compared to the SimpleCNN model.\n",
        "\n",
        "We can conclude that the ResNet50 model is learning well without overfitting. Compard to the SimpleCNN model it is much more accurate showing an improved ~21% accuracy. Since the losses are converging towards each other around epoch 3, we can see that the model appears to be learning properly with no overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kTcf1GbHKl3"
      },
      "source": [
        "# Q4. Run the following experiments and report the results.\n",
        "- Train and evaluate the `VGGTransfer` model on the CIFAR-10 dataset (for 5 epochs minimum).\n",
        "- Plot the train and test loss, as well as the test accuracy.\n",
        "- Analyze and report the results. Provide reasoning for your analysis. Make comparisons to the previous experiments when appropriate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-d40tEzuYv0",
        "outputId": "e788d6a5-574c-4a4c-e5f0-f214e639397c"
      },
      "outputs": [],
      "source": [
        "T = VGG16_Weights.DEFAULT.transforms()\n",
        "train_loader, test_loader = load_data(T, BATCH_SIZE)\n",
        "\n",
        "vgg_transfer = VGGTransfer(NUM_CLASSES).to(device)\n",
        "optimizer = torch.optim.Adam(vgg_transfer.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "train_losses, test_losses, accuracies = run_experiment(vgg_transfer, train_loader, test_loader, optimizer, criterion, device, EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "rQpOCgdZunkr",
        "outputId": "1f1f7ce4-b78d-49a5-cdd6-0ba127d7d83a"
      },
      "outputs": [],
      "source": [
        "plot_results(train_losses, test_losses, accuracies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DArH015oZ0ML"
      },
      "source": [
        "The results of running the VGG Model on the CIFAR-10 dataset through 5 epochs show that\n",
        "\n",
        "1.   The training loss very quickly drops from over 0.48 to 0.02\n",
        "2.   The test loss gradually increases from 0.40 to 0.49\n",
        "3.   The test accuracy stays around 0.86\n",
        "\n",
        "Through observing all of this data we can see that the VGG loss is clearly diverging against in between the 0th and 1st epoch which is a huge red flag. The divergence shows that there is significant signs of overfitting happening here.\n",
        "\n",
        "The accuracy here ends up initializing and staying around 0.86. There is a dip at epoch 2 and although this model seems like there's a high accuracy for it, this accuracy does not directly correlate to the training because there is severe overfitting in with the training and testing losses.\n",
        "\n",
        "We can conclude that the VGG model is learning poorly because there is significant overfitting visible in the losses. This suggests that we might have to change some parameters or structure of the VGG model up in order to eliminate the overfitting or use a different model instead."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
